"""
æ™ºèƒ½æ–‡æœ¬é¢„å¤„ç†å™¨ - ä¸“ä¸šçš„TTSæ–‡æœ¬é¢„å¤„ç†è§£å†³æ–¹æ¡ˆ

æä¾›å®Œæ•´çš„æ–‡æœ¬é¢„å¤„ç†åŠŸèƒ½ï¼Œä¸“ä¸ºTTSï¼ˆæ–‡æœ¬è½¬è¯­éŸ³ï¼‰ç³»ç»Ÿè®¾è®¡ï¼š

ä¸»è¦åŠŸèƒ½:
1. åŸºç¡€æ¸…ç† - ç§»é™¤é—®é¢˜å­—ç¬¦å’Œè¾¹ç•Œæƒ…å†µå¤„ç†
2. æ·±åº¦æ ‡å‡†åŒ– - æœ¯è¯­æ›¿æ¢å’Œå¤šè¯­è¨€è§„èŒƒåŒ–
3. æ™ºèƒ½åˆ†å‰² - å¤šç§åˆ†å‰²ç­–ç•¥è‡ªé€‚åº”é€‰æ‹©
4. è¯­è¨€æ£€æµ‹ - æ™ºèƒ½å¤šè¯­è¨€è¯†åˆ«
5. æ‰¹é‡å¤„ç† - æ”¯æŒå¤§è§„æ¨¡æ–‡æœ¬å¤„ç†

ç‰¹æ€§:
- é«˜æ€§èƒ½çš„æ­£åˆ™è¡¨è¾¾å¼ä¼˜åŒ–
- çµæ´»çš„åˆ†å‰²ç­–ç•¥é€‰æ‹©
- è¯¦ç»†çš„å¤„ç†ç»Ÿè®¡ä¿¡æ¯
- æ”¯æŒè‡ªå®šä¹‰é…ç½®å’Œæ‰©å±•
"""

import logging
import re
import time
from collections import defaultdict
from dataclasses import asdict, dataclass
from enum import Enum
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Tuple, Union


class SplitStrategy(Enum):
    """æ–‡æœ¬åˆ†å‰²ç­–ç•¥"""

    SENTENCE = "sentence"  # æŒ‰å¥å­åˆ†å‰²
    LENGTH = "length"  # æŒ‰é•¿åº¦åˆ†å‰²
    PUNCTUATION = "punctuation"  # æŒ‰æ ‡ç‚¹åˆ†å‰²
    ADAPTIVE = "adaptive"  # è‡ªé€‚åº”æ··åˆç­–ç•¥


class LanguageType(Enum):
    """è¯­è¨€ç±»å‹"""

    CHINESE = "zh"
    ENGLISH = "en"
    JAPANESE = "ja"
    KOREAN = "ko"
    FRENCH = "fr"
    SPANISH = "es"
    GERMAN = "de"
    MIXED = "mixed"
    UNKNOWN = "unknown"


@dataclass
class TextProcessingResult:
    """æ–‡æœ¬å¤„ç†ç»“æœ"""

    original_text: str  # åŸå§‹æ–‡æœ¬
    cleaned_text: str  # æ¸…ç†åæ–‡æœ¬
    segments: List[str]  # åˆ†å‰²åçš„ç‰‡æ®µ
    language: LanguageType  # æ£€æµ‹åˆ°çš„è¯­è¨€
    text_hash: str  # æ–‡æœ¬å“ˆå¸Œ
    metadata: Dict[str, Any]  # å…ƒæ•°æ®ä¿¡æ¯
    is_valid: bool  # æ˜¯å¦æœ‰æ•ˆ
    processing_time: float = 0.0  # å¤„ç†è€—æ—¶ï¼ˆç§’ï¼‰
    error_message: Optional[str] = None  # é”™è¯¯ä¿¡æ¯

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        result = asdict(self)
        result["language"] = self.language.value
        return result


class IntelligentTextProcessor:
    """
    æ™ºèƒ½æ–‡æœ¬é¢„å¤„ç†å™¨

    ä¸“ä¸šçš„TTSæ–‡æœ¬é¢„å¤„ç†è§£å†³æ–¹æ¡ˆï¼Œæä¾›é«˜æ•ˆã€å¯é çš„æ–‡æœ¬å¤„ç†åŠŸèƒ½ã€‚
    """

    # é—®é¢˜å­—ç¬¦æ˜ å°„
    PROBLEMATIC_CHARS = {
        "&": "å’Œ",
        "Â®": "",
        "â„¢": "",
        "Â©": "",
        "â„ƒ": "æ‘„æ°åº¦",
        "â„‰": "åæ°åº¦",
        "â‚¬": "æ¬§å…ƒ",
        "$": "ç¾å…ƒ",
        "Â£": "è‹±é•‘",
        "Â¥": "äººæ°‘å¸",
        "@": "è‰¾ç‰¹",
        "#": "äº•å·",
        # æ³¨æ„ï¼šç™¾åˆ†å·(%)ç°åœ¨ç”±_process_percentage()æ–¹æ³•æ™ºèƒ½å¤„ç†
    }

    # æœ¯è¯­æ›¿æ¢æ˜ å°„
    TERM_REPLACEMENTS = {
        "AI": "äººå·¥æ™ºèƒ½",
        "API": "åº”ç”¨ç¨‹åºæ¥å£",
        "URL": "ç½‘å€",
        "CPU": "ä¸­å¤®å¤„ç†å™¨",
        "GPU": "å›¾å½¢å¤„ç†å™¨",
        "iPhone": "è‹¹æœæ‰‹æœº",
        "iPad": "è‹¹æœå¹³æ¿",
        "Android": "å®‰å“",
        "Windows": "è§†çª—ç³»ç»Ÿ",
        "macOS": "è‹¹æœç³»ç»Ÿ",
    }

    # æ ‡ç‚¹ç¬¦å·æ˜ å°„
    PUNCTUATION_MAP = {
        "...": "ã€‚",
        "â€¦": "ã€‚",
        "!": "ï¼",
        "?": "ï¼Ÿ",
        ":": "ï¼š",
        ";": "ï¼›",
        ",": "ï¼Œ",
        ".": "ã€‚",
        '"': '"',
        "'": "'",
        "(": "ï¼ˆ",
        ")": "ï¼‰",
    }

    def __init__(
        self,
        max_text_length: int = 200,
        default_split_strategy: SplitStrategy = SplitStrategy.ADAPTIVE,
    ):
        """åˆå§‹åŒ–æ™ºèƒ½æ–‡æœ¬å¤„ç†å™¨"""
        # åŸºç¡€é…ç½®
        self.max_text_length = max_text_length
        self.default_split_strategy = default_split_strategy

        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        self._compile_regex_patterns()

        # åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
        self.logger = logging.getLogger(__name__)

        # æ€§èƒ½ç»Ÿè®¡
        self._processing_stats = defaultdict(int)

    def _compile_regex_patterns(self):
        """ç¼–è¯‘å¸¸ç”¨çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰"""
        # åŸºç¡€å­—ç¬¦æ¨¡å¼
        self.chinese_pattern = re.compile(r"[\u4e00-\u9fff]+")
        self.english_pattern = re.compile(r"[a-zA-Z]+")
        self.whitespace_pattern = re.compile(r"\s+")

        # æ–‡æœ¬æ ‡å‡†åŒ–æ¨¡å¼
        self.uppercase_pattern = re.compile(r"(?<!^)([A-Z])")
        self.alphanumeric_pattern = re.compile(
            r"(?<=[a-zA-Z])(?=\d)|(?<=\d)(?=[a-zA-Z])"
        )

        # è¯­è¨€æ£€æµ‹æ¨¡å¼
        self.japanese_pattern = re.compile(r"[\u3040-\u309f\u30a0-\u30ff]+")
        self.korean_pattern = re.compile(r"[\uac00-\ud7af]+")

        # æ¸…ç†æ¨¡å¼
        self.punctuation_only_pattern = re.compile(r"^[^\w\s]*$")
        self.single_digit_pattern = re.compile(r"\b\d\b")

        # æ‹¬å·å†…å®¹åˆ é™¤æ¨¡å¼
        self.parentheses_pattern = re.compile(r"\([^)]*\)")  # åœ†æ‹¬å·
        self.square_brackets_pattern = re.compile(r"\[[^\]]*\]")  # æ–¹æ‹¬å·
        self.curly_brackets_pattern = re.compile(r"\{[^}]*\}")  # èŠ±æ‹¬å·
        self.angle_brackets_pattern = re.compile(r"<[^>]*>")  # å°–æ‹¬å·
        self.chinese_brackets_pattern = re.compile(r"ï¼ˆ[^ï¼‰]*ï¼‰")  # ä¸­æ–‡åœ†æ‹¬å·
        self.chinese_square_brackets_pattern = re.compile(r"ã€[^ã€‘]*ã€‘")  # ä¸­æ–‡æ–¹æ‹¬å·

        # è‹±æ–‡ç¼©å†™æ¨¡å¼
        self.contractions_pattern = re.compile(
            r"\b(don't|won't|can't|n't|'re|'ve|'ll|'d|'m)\b", re.IGNORECASE
        )

    def clean_text_for_tts(self, text: str) -> str:
        """ç®€åŒ–çš„æ–‡æœ¬æ¸…ç†æ¥å£ï¼ˆå…¼å®¹ç°æœ‰ä»£ç ï¼‰"""
        result = self.process(text)
        return result.cleaned_text if result.is_valid else ""

    def validate_text(self, text: str) -> Tuple[bool, Optional[str]]:
        """æ–‡æœ¬éªŒè¯æ¥å£ï¼ˆå…¼å®¹ç°æœ‰ä»£ç ï¼‰"""
        if not text or not text.strip():
            return False, "æ–‡æœ¬ä¸ºç©º"
        if len(text) > self.max_text_length * 10:
            return False, f"æ–‡æœ¬è¿‡é•¿: {len(text)} å­—ç¬¦"
        return True, None

    def process(
        self,
        text: str,
        split_strategy: Optional[SplitStrategy] = None,
        target_language: Optional[LanguageType] = None,
        progress_callback: Optional[Callable[[str, float], None]] = None,
    ) -> TextProcessingResult:
        """
        æ™ºèƒ½æ–‡æœ¬å¤„ç†ä¸»å…¥å£

        Args:
            text: åŸå§‹æ–‡æœ¬
            split_strategy: åˆ†å‰²ç­–ç•¥
            target_language: ç›®æ ‡è¯­è¨€
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            TextProcessingResult: å¤„ç†ç»“æœ
        """
        start_time = time.time()

        def report_progress(stage: str, progress: float):
            if progress_callback:
                progress_callback(stage, progress)

        try:
            # 1. åŸºç¡€éªŒè¯
            report_progress("éªŒè¯è¾“å…¥", 0.1)
            if not text or not text.strip():
                return TextProcessingResult(
                    original_text=text,
                    cleaned_text="",
                    segments=[],
                    language=LanguageType.UNKNOWN,
                    text_hash="",
                    metadata={},
                    is_valid=False,
                    processing_time=time.time() - start_time,
                    error_message="æ–‡æœ¬ä¸ºç©º",
                )

            # 2. è¯­è¨€æ£€æµ‹
            report_progress("æ£€æµ‹è¯­è¨€", 0.2)
            detected_language = target_language or self._detect_language(text)

            # 3. åŸºç¡€æ¸…ç†
            report_progress("åŸºç¡€æ¸…ç†", 0.3)
            cleaned_text = self._basic_cleaning(text)

            # 4. æ·±åº¦æ ‡å‡†åŒ–
            report_progress("æ·±åº¦æ ‡å‡†åŒ–", 0.5)
            normalized_text = self._deep_normalization(cleaned_text, detected_language)

            # 5. æ™ºèƒ½åˆ†å‰²
            report_progress("æ™ºèƒ½åˆ†å‰²", 0.7)
            split_strategy = split_strategy or self.default_split_strategy
            segments = self._intelligent_split(
                normalized_text, split_strategy, detected_language
            )

            # 6. éªŒè¯ç»“æœ
            report_progress("éªŒè¯ç»“æœ", 0.8)
            is_valid, error_msg = self._validate_result(normalized_text, segments)

            # 7. æ„å»ºç»“æœ
            processing_time = time.time() - start_time
            result = TextProcessingResult(
                original_text=text,
                cleaned_text=normalized_text,
                segments=segments,
                language=detected_language,
                text_hash="",
                metadata={
                    "original_length": len(text),
                    "cleaned_length": len(normalized_text),
                    "segment_count": len(segments),
                    "split_strategy": split_strategy.value,
                    "processing_steps": [
                        "basic_cleaning",
                        "deep_normalization",
                        "intelligent_split",
                    ],
                },
                is_valid=is_valid,
                processing_time=processing_time,
                error_message=error_msg,
            )

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self._processing_stats["total_processed"] += 1
            self._processing_stats["total_processing_time"] += processing_time

            return result

        except Exception as e:
            processing_time = time.time() - start_time
            self.logger.error(f"æ–‡æœ¬å¤„ç†å¼‚å¸¸: {str(e)}")
            self._processing_stats["total_errors"] += 1

            return TextProcessingResult(
                original_text=text,
                cleaned_text="",
                segments=[],
                language=LanguageType.UNKNOWN,
                text_hash="",
                metadata={"error_type": type(e).__name__},
                is_valid=False,
                processing_time=processing_time,
                error_message=f"å¤„ç†å¼‚å¸¸: {str(e)}",
            )

    def _detect_language(self, text: str) -> LanguageType:
        """æ™ºèƒ½è¯­è¨€æ£€æµ‹"""
        if not text or not text.strip():
            return LanguageType.UNKNOWN

        text_stripped = text.strip()
        total_chars = len(text_stripped)

        # ä½¿ç”¨é¢„ç¼–è¯‘çš„æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œé«˜æ•ˆæ£€æµ‹
        chinese_matches = self.chinese_pattern.findall(text_stripped)
        english_matches = self.english_pattern.findall(text_stripped)
        japanese_matches = self.japanese_pattern.findall(text_stripped)
        korean_matches = self.korean_pattern.findall(text_stripped)

        # è®¡ç®—å­—ç¬¦æ•°é‡
        chinese_chars = sum(len(match) for match in chinese_matches)
        english_chars = sum(len(match) for match in english_matches)
        japanese_chars = sum(len(match) for match in japanese_matches)
        korean_chars = sum(len(match) for match in korean_matches)

        # è®¡ç®—æ¯”ä¾‹
        chinese_ratio = chinese_chars / total_chars if total_chars > 0 else 0
        english_ratio = english_chars / total_chars if total_chars > 0 else 0

        # è¯­è¨€åˆ¤æ–­é€»è¾‘
        if korean_chars > 0:
            return LanguageType.KOREAN
        elif japanese_chars > 0:
            return LanguageType.JAPANESE
        elif chinese_ratio > 0.3:
            if english_ratio > 0.2:
                return LanguageType.MIXED
            return LanguageType.CHINESE
        elif english_chars > 0:
            return LanguageType.ENGLISH
        else:
            return LanguageType.UNKNOWN

    def _basic_cleaning(self, text: str) -> str:
        """åŸºç¡€æ¸…ç†ï¼ˆåŒ…å«åˆ é™¤æ‹¬å·å†…å®¹ï¼‰"""
        if not text:
            return ""

        # ğŸš¨ ä¿®å¤ç¼–ç é—®é¢˜ï¼šé¦–å…ˆç§»é™¤éŸ³ä¹ç›¸å…³emojiå’Œå¯èƒ½å¯¼è‡´GBKç¼–ç é—®é¢˜çš„å­—ç¬¦
        original_text = text
        text = self._remove_problematic_unicode_chars(text)

        # 1. åˆ é™¤å„ç§æ‹¬å·å†…å®¹ï¼ˆTTSä¸éœ€è¦æ‹¬å·å†…çš„æ³¨é‡Šã€è¯´æ˜ç­‰ï¼‰
        text = self._remove_brackets_content(text)

        # 2. æ‰¹é‡æ›¿æ¢é—®é¢˜å­—ç¬¦
        for char, replacement in self.PROBLEMATIC_CHARS.items():
            if char in text:
                text = text.replace(char, replacement)

        # 3. å¤„ç†è¾¹ç•Œæƒ…å†µ
        if self.punctuation_only_pattern.match(text.strip()):
            return ""

        stripped_text = text.strip()
        if len(stripped_text) <= 1:
            return ""

        # 4. ç©ºç™½å­—ç¬¦æ¸…ç†
        text = self.whitespace_pattern.sub(" ", stripped_text)

        # 5. æœ€ç»ˆUnicodeæ¸…ç†ç¡®ä¿
        text = self._remove_problematic_unicode_chars(text)

        return text

    def _remove_brackets_content(self, text: str) -> str:
        """
        åˆ é™¤æ‹¬å·å†…å®¹

        åˆ é™¤å„ç§ç±»å‹çš„æ‹¬å·åŠå…¶å†…å®¹ï¼Œå› ä¸ºTTSé€šå¸¸ä¸éœ€è¦æœ—è¯»æ‹¬å·å†…çš„æ³¨é‡Šã€è¯´æ˜ç­‰å†…å®¹

        Args:
            text: åŸå§‹æ–‡æœ¬

        Returns:
            åˆ é™¤æ‹¬å·å†…å®¹åçš„æ–‡æœ¬
        """
        if not text:
            return ""

        # æŒ‰ä¼˜å…ˆçº§åˆ é™¤å„ç§æ‹¬å·å†…å®¹
        # 1. åˆ é™¤åœ†æ‹¬å·å†…å®¹ (æ³¨é‡Š)
        text = self.parentheses_pattern.sub("", text)

        # 2. åˆ é™¤æ–¹æ‹¬å·å†…å®¹ [å¤‡æ³¨]
        text = self.square_brackets_pattern.sub("", text)

        # 3. åˆ é™¤èŠ±æ‹¬å·å†…å®¹ {è¯´æ˜}
        text = self.curly_brackets_pattern.sub("", text)

        # 4. åˆ é™¤å°–æ‹¬å·å†…å®¹ <æ ‡ç­¾>
        text = self.angle_brackets_pattern.sub("", text)

        # 5. åˆ é™¤ä¸­æ–‡åœ†æ‹¬å·å†…å®¹ ï¼ˆæ³¨é‡Šï¼‰
        text = self.chinese_brackets_pattern.sub("", text)

        # 6. åˆ é™¤ä¸­æ–‡æ–¹æ‹¬å·å†…å®¹ ã€å¤‡æ³¨ã€‘
        text = self.chinese_square_brackets_pattern.sub("", text)

        # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = self.whitespace_pattern.sub(" ", text).strip()

        return text

    def _remove_problematic_unicode_chars(self, text: str) -> str:
        """
        ç§»é™¤å¯èƒ½å¯¼è‡´GBKç¼–ç é—®é¢˜çš„Unicodeå­—ç¬¦

        ä¸»è¦ç§»é™¤éŸ³ä¹ç›¸å…³emojiå’Œå…¶ä»–å¯èƒ½çš„é—®é¢˜å­—ç¬¦
        """
        if not text:
            return ""

        import re

        # ğŸš¨ å…³é”®ä¿®å¤ï¼šæ‰©å¤§Unicodeå­—ç¬¦æ¸…ç†èŒƒå›´ï¼Œç¡®ä¿å½»åº•ç§»é™¤é—®é¢˜å­—ç¬¦
        # ç§»é™¤æ‰€æœ‰å¯èƒ½å¯¼è‡´GBKç¼–ç é—®é¢˜çš„Unicodeå­—ç¬¦
        problematic_unicode_pattern = re.compile(
            r"[\U0001F000-\U0001F9FF"  # å…¨éƒ¨EmojiåŒºåŸŸ
            r"\U00002600-\U000026FF"  # æ‚é¡¹ç¬¦å·
            r"\U00002700-\U000027BF"  # è£…é¥°ç¬¦å·
            r"\U0000FE00-\U0000FE0F"  # å˜ä½“é€‰æ‹©ç¬¦
            r"\U0001F1E0-\U0001F1FF"  # åŒºåŸŸæŒ‡ç¤ºç¬¦
            r"\U0000200D"  # é›¶å®½è¿æ¥ç¬¦
            r"\U0000FE0F"  # å˜ä½“é€‰æ‹©ç¬¦-16
            r"\U0000FE0E"  # å˜ä½“é€‰æ‹©ç¬¦-15
            r"\U0001F3FB-\U0001F3FF"  # è‚¤è‰²ä¿®é¥°ç¬¦
            r"\U0001FA70-\U0001FAFF"  # æ‰©å±•ç¬¦å·å’Œè±¡å½¢æ–‡å­—
            r"\U00010000-\U0001FFFF]"  # æ‰€æœ‰è¡¥å……å¹³é¢å­—ç¬¦
        )

        # ç§»é™¤åŒ¹é…çš„å­—ç¬¦
        cleaned_text = problematic_unicode_pattern.sub("", text)

        # é¢å¤–çš„å®‰å…¨æ¸…ç†ï¼šé€å­—ç¬¦æ£€æŸ¥å¹¶ç§»é™¤æ‰€æœ‰éåŸºæœ¬å¤šæ–‡ç§å¹³é¢çš„å­—ç¬¦
        safe_chars = []
        for char in cleaned_text:
            char_code = ord(char)
            # åªä¿ç•™åŸºæœ¬å¤šæ–‡ç§å¹³é¢çš„å­—ç¬¦ (U+0000 åˆ° U+FFFF)
            # æ’é™¤ä»£ç†å¯¹åŒºåŸŸ (U+D800 åˆ° U+DFFF)
            if char_code <= 0xFFFF and not (0xD800 <= char_code <= 0xDFFF):
                # å†æ¬¡æ£€æŸ¥æ˜¯å¦ä¸ºGBKå¯ç¼–ç å­—ç¬¦
                try:
                    char.encode("gbk")
                    safe_chars.append(char)
                except UnicodeEncodeError:
                    # å¦‚æœGBKæ— æ³•ç¼–ç åˆ™è·³è¿‡
                    continue
            # è·³è¿‡æ‰€æœ‰é«˜ä½Unicodeå­—ç¬¦

        result = "".join(safe_chars)

        # ç¡®ä¿æ–‡æœ¬ä½¿ç”¨UTF-8ç¼–ç ä½†å…¼å®¹GBK
        try:
            result = result.encode("utf-8", errors="ignore").decode("utf-8")
        except Exception:
            pass  # å¦‚æœç¼–ç è½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡æœ¬

        return result.strip()

    def _gentle_bracket_removal(self, text: str) -> str:
        """
        æ¸©å’Œçš„æ‹¬å·ç§»é™¤ç­–ç•¥

        åªç§»é™¤æ˜æ˜¾çš„æ³¨é‡Šæ€§æ‹¬å·å†…å®¹ï¼Œä¿ç•™å¯èƒ½åŒ…å«é‡è¦ä¿¡æ¯çš„å†…å®¹
        """
        if not text:
            return ""

        import re

        original_text = text

        # é¦–å…ˆç§»é™¤Unicodeé—®é¢˜å­—ç¬¦
        text = self._remove_problematic_unicode_chars(text)

        # åªç§»é™¤æ˜æ˜¾çš„æ³¨é‡Šæ€§å†…å®¹
        # ç§»é™¤ç¿»è¯‘æ ‡æ³¨ (å­—å¹•ç¿»è¯‘ï¼šxxx)
        text = re.sub(r"\(å­—å¹•ç¿»è¯‘[ï¼š:][^)]*\)", "", text)
        # ç§»é™¤è¯´æ˜æ€§å†…å®¹ (è¯´æ˜ï¼šxxx)ã€(æ³¨ï¼šxxx)ã€(å¤‡æ³¨ï¼šxxx)
        text = re.sub(r"\([è¯´æ³¨å¤‡][æ˜æ³¨ï¼š][ï¼š:][^)]*\)", "", text)
        # ç§»é™¤å•å…ƒé¢„è§ˆæ ‡æ³¨
        text = re.sub(r"\(å•å…ƒ[^)]*é¢„è§ˆ[^)]*\)", "", text)

        # æ¸…ç†ç©ºç™½
        text = self.whitespace_pattern.sub(" ", text).strip()

        # å¦‚æœæ¸…ç†åä»ç„¶ä¸ºç©ºï¼Œä»åŸæ–‡æœ¬ä¸­æå–æœ‰æ„ä¹‰çš„å†…å®¹
        if not text:
            # ä»åŸæ–‡æœ¬ä¸­æå–éæ‹¬å·å†…å®¹
            cleaned_original = self._remove_problematic_unicode_chars(original_text)
            # ç§»é™¤æ‹¬å·å†…å®¹ä½†ä¿ç•™å¤–éƒ¨å†…å®¹
            no_brackets = re.sub(r"[\(ï¼ˆ][^\)ï¼‰]*[\)ï¼‰]", " ", cleaned_original)
            text = self.whitespace_pattern.sub(" ", no_brackets).strip()

        # æœ€ç»ˆUnicodeæ¸…ç†
        text = self._remove_problematic_unicode_chars(text)

        return text

    def _deep_normalization(self, text: str, language: LanguageType) -> str:
        """æ·±åº¦æ ‡å‡†åŒ–"""
        if not text:
            return ""

        # æœ¯è¯­æ›¿æ¢
        for term, replacement in self.TERM_REPLACEMENTS.items():
            if term in text:
                text = text.replace(term, replacement)

        # å¤§å†™å­—æ¯å‰æ’å…¥ç©ºæ ¼
        text = self.uppercase_pattern.sub(r" \1", text)

        # å­—æ¯æ•°å­—é—´æ’å…¥ç©ºæ ¼
        text = self.alphanumeric_pattern.sub(" ", text)

        # ç ´æŠ˜å·æ™ºèƒ½å¤„ç†ï¼ˆåœ¨è¯­è¨€ç‰¹å®šå¤„ç†ä¹‹å‰ï¼Œé¿å…æ•°å­—è¢«è½¬æ¢ï¼‰
        text = self._process_dash(text)

        # ç™¾åˆ†æ¯”æ™ºèƒ½å¤„ç†ï¼ˆåœ¨æ•°å­—è½¬æ¢ä¹‹å‰å¤„ç†ï¼‰
        text = self._process_percentage(text)

        # è¯­è¨€ç‰¹å®šå¤„ç†
        if language in (LanguageType.CHINESE, LanguageType.MIXED):
            text = self._chinese_normalization(text)
        elif language == LanguageType.ENGLISH:
            text = self._english_normalization(text)

        # æ ‡ç‚¹ç¬¦å·æ ‡å‡†åŒ–
        for punct, replacement in self.PUNCTUATION_MAP.items():
            if punct in text:
                text = text.replace(punct, replacement)

        # æœ€ç»ˆæ¸…ç†
        text = self.whitespace_pattern.sub(" ", text).strip()
        return text

    def _process_dash(self, text: str) -> str:
        """
        æ™ºèƒ½å¤„ç†ç ´æŠ˜å·

        è§„åˆ™ï¼š
        1. å¦‚æœç ´æŠ˜å·ï¼ˆâ€“ æˆ– -ï¼‰å‰å5ä¸ªå­—ç¬¦å†…éƒ½æœ‰æ•°å­—ï¼Œè½¬æ¢ä¸º"è‡³"ï¼ˆè¡¨ç¤ºèŒƒå›´ï¼‰
        2. å¦åˆ™è½¬æ¢ä¸ºé€—å·ï¼ˆï¼Œï¼‰
        3. å¦‚æœå‰å5ä¸ªå­—ç¬¦ä¸­æœ‰æ ‡ç‚¹ç¬¦å·ï¼Œåˆ™ä»æ ‡ç‚¹ç¬¦å·åé¢ï¼ˆæˆ–å‰é¢ï¼‰çš„å­—ç¬¦å¼€å§‹åˆ°ç ´æŠ˜å·çš„èŒƒå›´å†…æ£€æŸ¥æ•°å­—
        4. ä¿æŒç ´æŠ˜å·å‰åçš„ç©ºæ ¼æ•°é‡

        ä¾‹å¦‚ï¼š
        - "é—®é¢˜1 â€“ é—®é¢˜3" â†’ "é—®é¢˜1 è‡³ é—®é¢˜3"
        - "10-15ç‚¹" â†’ "10è‡³15ç‚¹"
        - "å•å…ƒ 1  â€“  é¢„è§ˆ" â†’ "å•å…ƒ 1  ï¼Œ  é¢„è§ˆ"
        - "ç¬¬1ç« ï¼Œå¯¼è®º â€“ ç¬¬2ç« " â†’ "ç¬¬1ç« ï¼Œå¯¼è®º è‡³ ç¬¬2ç« "
        """
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç ´æŠ˜å·ï¼ˆçŸ­ç ´æŠ˜å·æˆ–é•¿ç ´æŠ˜å·ï¼‰
        if "â€“" not in text and "-" not in text:
            return text

        # æŸ¥æ‰¾æ‰€æœ‰ç ´æŠ˜å·çš„ä½ç½®ï¼ˆä¼˜å…ˆå¤„ç†é•¿ç ´æŠ˜å·ï¼Œç„¶åå¤„ç†çŸ­ç ´æŠ˜å·ï¼‰
        result = []
        last_pos = 0

        while True:
            # åŒæ—¶æŸ¥æ‰¾é•¿ç ´æŠ˜å·å’ŒçŸ­ç ´æŠ˜å·ï¼Œé€‰æ‹©æœ€è¿‘çš„ä¸€ä¸ª
            long_dash_pos = text.find("â€“", last_pos)
            short_dash_pos = text.find("-", last_pos)

            # ç¡®å®šä¸‹ä¸€ä¸ªè¦å¤„ç†çš„ç ´æŠ˜å·ä½ç½®å’Œç±»å‹
            if long_dash_pos == -1 and short_dash_pos == -1:
                # æ²¡æœ‰æ›´å¤šç ´æŠ˜å·ï¼Œæ·»åŠ å‰©ä½™æ–‡æœ¬
                result.append(text[last_pos:])
                break
            elif long_dash_pos == -1:
                dash_pos = short_dash_pos
                dash_char = "-"
            elif short_dash_pos == -1:
                dash_pos = long_dash_pos
                dash_char = "â€“"
            else:
                # ä¸¤ç§ç ´æŠ˜å·éƒ½å­˜åœ¨ï¼Œé€‰æ‹©ä½ç½®æ›´é å‰çš„
                if long_dash_pos < short_dash_pos:
                    dash_pos = long_dash_pos
                    dash_char = "â€“"
                else:
                    dash_pos = short_dash_pos
                    dash_char = "-"

            # è·å–ç ´æŠ˜å·å‰åçš„ç©ºæ ¼
            spaces_before = ""
            spaces_after = ""

            # è®¡ç®—ç ´æŠ˜å·å‰çš„ç©ºæ ¼
            i = dash_pos - 1
            while i >= 0 and text[i] == " ":
                spaces_before = " " + spaces_before
                i -= 1
            # i+1 ç°åœ¨æ˜¯éç©ºæ ¼å­—ç¬¦çš„ç»“æŸä½ç½®

            # è®¡ç®—ç ´æŠ˜å·åçš„ç©ºæ ¼
            j = dash_pos + 1
            while j < len(text) and text[j] == " ":
                spaces_after += " "
                j += 1
            # j ç°åœ¨æ˜¯éç©ºæ ¼å­—ç¬¦çš„å¼€å§‹ä½ç½®

            # æ·»åŠ ç ´æŠ˜å·ä¹‹å‰çš„æ–‡æœ¬ï¼ˆä¸åŒ…æ‹¬å‰é¢çš„ç©ºæ ¼ï¼‰
            result.append(text[last_pos : i + 1])

            # æ£€æŸ¥ç ´æŠ˜å·å‰åæ˜¯å¦æœ‰æ•°å­—
            has_digit_before = self._check_digit_in_range(
                text, dash_pos, direction="before"
            )
            has_digit_after = self._check_digit_in_range(
                text, dash_pos, direction="after"
            )

            # æ ¹æ®è§„åˆ™é€‰æ‹©æ›¿æ¢å†…å®¹ï¼Œä¿æŒåŸæœ‰ç©ºæ ¼
            if has_digit_before and has_digit_after:
                result.append(f"{spaces_before}è‡³{spaces_after}")
            else:
                result.append(f"{spaces_before}ï¼Œ{spaces_after}")

            # æ›´æ–°ä½ç½®åˆ°ç ´æŠ˜å·åç©ºæ ¼çš„ç»“æŸä½ç½®
            last_pos = j

        return "".join(result)

    def _check_digit_in_range(self, text: str, dash_pos: int, direction: str) -> bool:
        """
        æ£€æŸ¥ç ´æŠ˜å·å‰å5ä¸ªå­—ç¬¦èŒƒå›´å†…æ˜¯å¦æœ‰æ•°å­—
        å¦‚æœæœ‰æ ‡ç‚¹ç¬¦å·ï¼Œåˆ™ä»æ ‡ç‚¹ç¬¦å·åé¢ï¼ˆæˆ–å‰é¢ï¼‰çš„å­—ç¬¦å¼€å§‹è®¡ç®—

        Args:
            text: å®Œæ•´æ–‡æœ¬
            dash_pos: ç ´æŠ˜å·ä½ç½®
            direction: 'before' æˆ– 'after'

        Returns:
            bool: æ˜¯å¦åŒ…å«æ•°å­—
        """
        # å®šä¹‰å¸¸è§æ ‡ç‚¹ç¬¦å·
        punctuation_chars = set('ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š,.:;!?()ï¼ˆï¼‰[]ã€ã€‘{}""' "<>")

        if direction == "before":
            # æ£€æŸ¥ç ´æŠ˜å·å‰é¢5ä¸ªå­—ç¬¦
            start_pos = max(0, dash_pos - 5)
            check_text = text[start_pos:dash_pos]

            # ä»å³å‘å·¦æŸ¥æ‰¾æœ€è¿‘çš„æ ‡ç‚¹ç¬¦å·
            last_punct_pos = -1
            for i in range(len(check_text) - 1, -1, -1):
                if check_text[i] in punctuation_chars:
                    last_punct_pos = i
                    break

            # å¦‚æœæ‰¾åˆ°æ ‡ç‚¹ç¬¦å·ï¼Œåªæ£€æŸ¥æ ‡ç‚¹ç¬¦å·ä¹‹åçš„éƒ¨åˆ†
            if last_punct_pos != -1:
                check_text = check_text[last_punct_pos + 1 :]

        else:  # direction == 'after'
            # æ£€æŸ¥ç ´æŠ˜å·åé¢5ä¸ªå­—ç¬¦
            end_pos = min(
                len(text), dash_pos + 6
            )  # +1 è·³è¿‡ç ´æŠ˜å·æœ¬èº«ï¼Œ+5 æ£€æŸ¥åé¢5ä¸ªå­—ç¬¦
            check_text = text[dash_pos + 1 : end_pos]

            # ä»å·¦å‘å³æŸ¥æ‰¾æœ€è¿‘çš„æ ‡ç‚¹ç¬¦å·
            first_punct_pos = -1
            for i in range(len(check_text)):
                if check_text[i] in punctuation_chars:
                    first_punct_pos = i
                    break

            # å¦‚æœæ‰¾åˆ°æ ‡ç‚¹ç¬¦å·ï¼Œåªæ£€æŸ¥æ ‡ç‚¹ç¬¦å·ä¹‹å‰çš„éƒ¨åˆ†
            if first_punct_pos != -1:
                check_text = check_text[:first_punct_pos]

        # æ£€æŸ¥å¤„ç†åçš„æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«æ•°å­—
        return any(char.isdigit() for char in check_text)

    def _process_percentage(self, text: str) -> str:
        """
        æ™ºèƒ½å¤„ç†ç™¾åˆ†æ¯”

        è§„åˆ™ï¼š
        1. æ•°å­—+%ç¬¦å· â†’ ç™¾åˆ†ä¹‹+ä¸­æ–‡æ•°å­—
        2. ä¾‹å¦‚ï¼š20% â†’ ç™¾åˆ†ä¹‹äºŒåï¼Œ5% â†’ ç™¾åˆ†ä¹‹äº”ï¼Œ100% â†’ ç™¾åˆ†ä¹‹ä¸€ç™¾
        3. æ”¯æŒå°æ•°ç™¾åˆ†æ¯”ï¼š20.5% â†’ ç™¾åˆ†ä¹‹äºŒåç‚¹äº”
        4. æ”¯æŒå¸¦ç©ºæ ¼çš„æƒ…å†µï¼š20 % â†’ ç™¾åˆ†ä¹‹äºŒå

        ä¾‹å¦‚ï¼š
        - "å‡†ç¡®ç‡è¾¾åˆ°95%" â†’ "å‡†ç¡®ç‡è¾¾åˆ°ç™¾åˆ†ä¹‹ä¹åäº”"
        - "å¢é•¿äº†20.5%" â†’ "å¢é•¿äº†ç™¾åˆ†ä¹‹äºŒåç‚¹äº”"
        - "åªæœ‰ 5 %" â†’ "åªæœ‰ç™¾åˆ†ä¹‹äº”"
        """
        if "%" not in text:
            return text

        import re

        # åŒ¹é…æ•°å­—+ç™¾åˆ†å·çš„æ¨¡å¼ï¼ˆæ”¯æŒå°æ•°å’Œç©ºæ ¼ï¼‰
        # åŒ¹é…æ•´æ•°ç™¾åˆ†æ¯”ï¼š20%ã€20 %
        # åŒ¹é…å°æ•°ç™¾åˆ†æ¯”ï¼š20.5%ã€20.5 %
        percentage_pattern = re.compile(r"(\d+(?:\.\d+)?)\s*%")

        def replace_percentage(match):
            number_str = match.group(1)

            # å¤„ç†å°æ•°æƒ…å†µ
            if "." in number_str:
                integer_part, decimal_part = number_str.split(".")
                result = "ç™¾åˆ†ä¹‹" + self._convert_number_to_chinese(integer_part)
                result += "ç‚¹" + self._convert_decimal_to_chinese(decimal_part)
            else:
                # å¤„ç†æ•´æ•°æƒ…å†µ
                result = "ç™¾åˆ†ä¹‹" + self._convert_number_to_chinese(number_str)

            return result

        text = percentage_pattern.sub(replace_percentage, text)
        return text

    def _convert_number_to_chinese(self, number_str: str) -> str:
        """
        å°†é˜¿æ‹‰ä¼¯æ•°å­—è½¬æ¢ä¸ºä¸­æ–‡æ•°å­—

        æ”¯æŒï¼š
        - ä¸ªä½æ•°ï¼š0-9 â†’ é›¶-ä¹
        - åä½æ•°ï¼š10-99 â†’ å-ä¹åä¹
        - ç™¾ä½æ•°ï¼š100-999 â†’ ä¸€ç™¾-ä¹ç™¾ä¹åä¹
        - æ›´å¤§æ•°å­—ï¼šé€ä½è½¬æ¢
        """
        if not number_str.isdigit():
            return number_str

        # åŸºç¡€æ•°å­—æ˜ å°„
        digit_map = {
            "0": "é›¶",
            "1": "ä¸€",
            "2": "äºŒ",
            "3": "ä¸‰",
            "4": "å››",
            "5": "äº”",
            "6": "å…­",
            "7": "ä¸ƒ",
            "8": "å…«",
            "9": "ä¹",
        }

        num = int(number_str)

        # ç‰¹æ®Šå¤„ç†0
        if num == 0:
            return "é›¶"

        # å¤„ç†1-99
        if num < 100:
            if num < 10:
                return digit_map[str(num)]
            elif num == 10:
                return "å"
            elif num < 20:
                return "å" + digit_map[str(num % 10)]
            else:
                tens = num // 10
                ones = num % 10
                result = digit_map[str(tens)] + "å"
                if ones > 0:
                    result += digit_map[str(ones)]
                return result

        # å¤„ç†100-999
        elif num < 1000:
            hundreds = num // 100
            remainder = num % 100
            result = digit_map[str(hundreds)] + "ç™¾"

            if remainder == 0:
                return result
            elif remainder < 10:
                result += "é›¶" + digit_map[str(remainder)]
            elif remainder == 10:
                result += "ä¸€å"
            elif remainder < 20:
                result += "ä¸€å" + digit_map[str(remainder % 10)]
            else:
                tens = remainder // 10
                ones = remainder % 10
                result += digit_map[str(tens)] + "å"
                if ones > 0:
                    result += digit_map[str(ones)]
            return result

        # å¯¹äºæ›´å¤§çš„æ•°å­—ï¼Œç®€åŒ–å¤„ç†ï¼šé€ä½è½¬æ¢
        else:
            return "".join(digit_map.get(digit, digit) for digit in number_str)

    def _convert_decimal_to_chinese(self, decimal_str: str) -> str:
        """
        å°†å°æ•°éƒ¨åˆ†è½¬æ¢ä¸ºä¸­æ–‡

        ä¾‹å¦‚ï¼š
        - "5" â†’ "äº”"
        - "25" â†’ "äºŒäº”"
        """
        digit_map = {
            "0": "é›¶",
            "1": "ä¸€",
            "2": "äºŒ",
            "3": "ä¸‰",
            "4": "å››",
            "5": "äº”",
            "6": "å…­",
            "7": "ä¸ƒ",
            "8": "å…«",
            "9": "ä¹",
        }

        return "".join(digit_map.get(digit, digit) for digit in decimal_str)

    def _chinese_normalization(self, text: str) -> str:
        """ä¸­æ–‡æ–‡æœ¬ç‰¹æ®Šå¤„ç†"""
        if not text:
            return ""

        # æ•°å­—è½¬ä¸­æ–‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
        number_map = {
            "0": "é›¶",
            "1": "ä¸€",
            "2": "äºŒ",
            "3": "ä¸‰",
            "4": "å››",
            "5": "äº”",
            "6": "å…­",
            "7": "ä¸ƒ",
            "8": "å…«",
            "9": "ä¹",
        }

        def replace_single_digit(match):
            digit = match.group()
            return number_map.get(digit, digit)

        text = self.single_digit_pattern.sub(replace_single_digit, text)
        return text

    def _english_normalization(self, text: str) -> str:
        """è‹±æ–‡æ–‡æœ¬ç‰¹æ®Šå¤„ç†"""
        if not text:
            return ""

        def expand_contraction(match):
            contraction = match.group().lower()
            expansions = {
                "don't": "do not",
                "won't": "will not",
                "can't": "cannot",
                "n't": " not",
                "'re": " are",
                "'ve": " have",
                "'ll": " will",
                "'d": " would",
                "'m": " am",
            }
            return expansions.get(contraction, contraction)

        text = self.contractions_pattern.sub(expand_contraction, text)
        return text

    def _intelligent_split(
        self, text: str, strategy: SplitStrategy, language: LanguageType
    ) -> List[str]:
        """æ™ºèƒ½åˆ†å‰²ç­–ç•¥"""
        if not text or len(text) <= self.max_text_length:
            return [text] if text else []

        if strategy == SplitStrategy.ADAPTIVE:
            return self._adaptive_split(text, language)
        elif strategy == SplitStrategy.SENTENCE:
            return self._sentence_split(text)
        elif strategy == SplitStrategy.LENGTH:
            return self._length_split(text)
        elif strategy == SplitStrategy.PUNCTUATION:
            return self._punctuation_split(text)
        else:
            return [text]

    def _adaptive_split(self, text: str, language: LanguageType) -> List[str]:
        """è‡ªé€‚åº”åˆ†å‰²ç­–ç•¥"""
        # é¦–å…ˆå°è¯•å¥å­åˆ†å‰²
        sentence_segments = self._sentence_split(text)

        # æ£€æŸ¥å¥å­åˆ†å‰²æ•ˆæœ
        if all(len(seg) <= self.max_text_length for seg in sentence_segments):
            return sentence_segments

        # å¯¹é•¿å¥å­è¿›è¡ŒäºŒæ¬¡åˆ†å‰²
        final_segments = []
        for segment in sentence_segments:
            if len(segment) <= self.max_text_length:
                final_segments.append(segment)
            else:
                # ä½¿ç”¨é•¿åº¦åˆ†å‰²
                final_segments.extend(self._length_split(segment))

        return final_segments

    def _sentence_split(self, text: str) -> List[str]:
        """æŒ‰å¥å­åˆ†å‰²"""
        sentences = []
        current_sentence = ""

        for char in text:
            current_sentence += char
            if char in "ã€‚ï¼ï¼Ÿï¼›.!?;":
                if current_sentence.strip():
                    sentences.append(current_sentence.strip())
                current_sentence = ""

        if current_sentence.strip():
            sentences.append(current_sentence.strip())

        return sentences

    def _punctuation_split(self, text: str) -> List[str]:
        """æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å‰²"""
        segments = []
        current_segment = ""

        for char in text:
            current_segment += char
            if char in "ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š,.:;!?":
                if current_segment.strip():
                    segments.append(current_segment.strip())
                current_segment = ""

        if current_segment.strip():
            segments.append(current_segment.strip())

        return segments

    def _length_split(self, text: str) -> List[str]:
        """æŒ‰é•¿åº¦æ™ºèƒ½åˆ†å‰²"""
        segments = []
        current_segment = ""
        words = text.split()

        for word in words:
            test_segment = current_segment + " " + word if current_segment else word

            if len(test_segment) <= self.max_text_length:
                current_segment = test_segment
            else:
                if current_segment:
                    segments.append(current_segment.strip())
                current_segment = word

        if current_segment:
            segments.append(current_segment.strip())

        return segments

    def _validate_result(
        self, text: str, segments: List[str]
    ) -> Tuple[bool, Optional[str]]:
        """éªŒè¯å¤„ç†ç»“æœ"""
        if not text.strip():
            return False, "å¤„ç†åæ–‡æœ¬ä¸ºç©º"

        if not segments:
            return False, "åˆ†å‰²åæ— æœ‰æ•ˆç‰‡æ®µ"

        for i, segment in enumerate(segments):
            if len(segment) > self.max_text_length:
                return (
                    False,
                    f"ç‰‡æ®µ {i+1} é•¿åº¦è¶…é™: {len(segment)} > {self.max_text_length}",
                )

        return True, None

    def batch_process(
        self,
        texts: List[str],
        split_strategy: Optional[SplitStrategy] = None,
        target_language: Optional[LanguageType] = None,
        progress_callback: Optional[Callable[[int, int], None]] = None,
    ) -> List[TextProcessingResult]:
        """æ‰¹é‡å¤„ç†æ–‡æœ¬åˆ—è¡¨"""
        results = []
        total = len(texts)

        for i, text in enumerate(texts):
            try:
                result = self.process(text, split_strategy, target_language)
                results.append(result)

                if progress_callback:
                    progress_callback(i + 1, total)

            except Exception as e:
                self.logger.error(f"æ‰¹é‡å¤„ç†ç¬¬{i+1}é¡¹å¤±è´¥: {str(e)}")
                error_result = TextProcessingResult(
                    original_text=text,
                    cleaned_text="",
                    segments=[],
                    language=LanguageType.UNKNOWN,
                    text_hash="",
                    metadata={},
                    is_valid=False,
                    error_message=str(e),
                )
                results.append(error_result)

        return results

    @classmethod
    def create_fast_processor(cls) -> "IntelligentTextProcessor":
        """åˆ›å»ºå¿«é€Ÿå¤„ç†å™¨"""
        return cls(max_text_length=100, default_split_strategy=SplitStrategy.LENGTH)

    @classmethod
    def create_quality_processor(cls) -> "IntelligentTextProcessor":
        """åˆ›å»ºé«˜è´¨é‡å¤„ç†å™¨"""
        return cls(max_text_length=300, default_split_strategy=SplitStrategy.ADAPTIVE)

    @classmethod
    def create_batch_processor(cls) -> "IntelligentTextProcessor":
        """åˆ›å»ºæ‰¹é‡å¤„ç†å™¨"""
        return cls(max_text_length=200, default_split_strategy=SplitStrategy.ADAPTIVE)

    @classmethod
    def create_minimal_processor(cls) -> "IntelligentTextProcessor":
        """åˆ›å»ºæœ€å°åŒ–å¤„ç†å™¨"""
        return cls(max_text_length=150, default_split_strategy=SplitStrategy.LENGTH)

    def get_processing_statistics(self) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        total_processed = self._processing_stats["total_processed"]
        total_time = self._processing_stats["total_processing_time"]

        return {
            "total_processed": total_processed,
            "total_errors": self._processing_stats["total_errors"],
            "total_processing_time": total_time,
            "average_processing_time": (
                total_time / total_processed if total_processed > 0 else 0.0
            ),
            "success_rate": (
                (total_processed - self._processing_stats["total_errors"])
                / total_processed
                if total_processed > 0
                else 0.0
            ),
        }


# å…¨å±€å¤„ç†å™¨å®ä¾‹
_global_processor: Optional[IntelligentTextProcessor] = None


def get_global_processor() -> IntelligentTextProcessor:
    """è·å–å…¨å±€å¤„ç†å™¨å®ä¾‹"""
    global _global_processor
    if _global_processor is None:
        _global_processor = IntelligentTextProcessor()
    return _global_processor


def quick_clean_text(text: str) -> str:
    """
    å¿«é€Ÿæ–‡æœ¬æ¸…ç†ï¼ˆæœ€å¸¸ç”¨çš„åŸºç¡€åŠŸèƒ½ï¼‰

    ä¸“é—¨ç”¨äºTTSå‰çš„æ–‡æœ¬æ¸…ç†ï¼Œä¼šè‡ªåŠ¨åˆ é™¤ï¼š
    - å„ç§æ‹¬å·å†…å®¹ï¼š(æ³¨é‡Š)ã€[å¤‡æ³¨]ã€{è¯´æ˜}ã€<æ ‡ç­¾>ã€ï¼ˆä¸­æ–‡ï¼‰ã€ã€ä¸­æ–‡ã€‘
    - é—®é¢˜å­—ç¬¦ï¼š&ã€Â®ã€â„¢ã€Â©ç­‰ç‰¹æ®Šç¬¦å·
    - å¤šä½™ç©ºç™½å­—ç¬¦

    ä½¿ç”¨åœºæ™¯ï¼š
    - å¿«é€Ÿæ¸…ç†ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
    - é¢„å¤„ç†TTSæ–‡æœ¬ï¼Œå»é™¤ä¸éœ€è¦æœ—è¯»çš„å†…å®¹
    - ä½œä¸ºå…¶ä»–å¤„ç†æ­¥éª¤çš„å‰ç½®æ¸…ç†

    Args:
        text: éœ€è¦æ¸…ç†çš„åŸå§‹æ–‡æœ¬

    Returns:
        æ¸…ç†åçš„æ–‡æœ¬ï¼Œé€‚åˆç›´æ¥ç”¨äºTTS

    Example:
        >>> quick_clean_text("è¿™æ˜¯AIæŠ€æœ¯(äººå·¥æ™ºèƒ½)çš„æ¼”ç¤º[å¤‡æ³¨]ã€‚")
        "è¿™æ˜¯AIæŠ€æœ¯çš„æ¼”ç¤ºã€‚"
    """
    processor = get_global_processor()
    return processor._basic_cleaning(text)


def process_text(
    text: str,
    split_strategy: Optional[SplitStrategy] = None,
    target_language: Optional[LanguageType] = None,
) -> TextProcessingResult:
    """
    å®Œæ•´æ–‡æœ¬å¤„ç†ï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼Œæ¨èä½¿ç”¨ï¼‰

    æä¾›å®Œæ•´çš„TTSæ–‡æœ¬é¢„å¤„ç†æµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
    - åŸºç¡€æ¸…ç†ï¼šåˆ é™¤æ‹¬å·å†…å®¹ã€é—®é¢˜å­—ç¬¦
    - æ·±åº¦æ ‡å‡†åŒ–ï¼šæœ¯è¯­æ›¿æ¢ï¼ˆAIâ†’äººå·¥æ™ºèƒ½ï¼‰ã€æ ¼å¼è§„èŒƒåŒ–
    - æ™ºèƒ½åˆ†å‰²ï¼šæ ¹æ®ç­–ç•¥åˆ†å‰²ä¸ºé€‚åˆTTSçš„ç‰‡æ®µ
    - è¯­è¨€æ£€æµ‹ï¼šè‡ªåŠ¨è¯†åˆ«æ–‡æœ¬è¯­è¨€ç±»å‹

    ä½¿ç”¨åœºæ™¯ï¼š
    - å•ä¸ªæ–‡æœ¬çš„å®Œæ•´é¢„å¤„ç†
    - éœ€è¦è¯¦ç»†å¤„ç†ä¿¡æ¯å’Œç»Ÿè®¡æ•°æ®
    - å¯¹å¤„ç†è´¨é‡è¦æ±‚è¾ƒé«˜çš„åœºæ™¯

    Args:
        text: åŸå§‹æ–‡æœ¬
        split_strategy: åˆ†å‰²ç­–ç•¥ï¼ˆè‡ªé€‚åº”/å¥å­/é•¿åº¦/æ ‡ç‚¹ï¼‰
        target_language: ç›®æ ‡è¯­è¨€ï¼ˆè‡ªåŠ¨æ£€æµ‹å¦‚æœä¸ºNoneï¼‰

    Returns:
        TextProcessingResult: åŒ…å«å¤„ç†ç»“æœã€ç»Ÿè®¡ä¿¡æ¯ã€å…ƒæ•°æ®çš„å®Œæ•´å¯¹è±¡

    Example:
        >>> result = process_text("è¿™æ˜¯AIæŠ€æœ¯(æ³¨é‡Š)æ¼”ç¤ºã€‚å¾ˆé•¿çš„æ–‡æœ¬ä¼šè¢«æ™ºèƒ½åˆ†å‰²ã€‚")
        >>> print(result.cleaned_text)  # "è¿™æ˜¯äººå·¥æ™ºèƒ½æ¼”ç¤ºã€‚å¾ˆé•¿çš„æ–‡æœ¬ä¼šè¢«æ™ºèƒ½åˆ†å‰²ã€‚"
        >>> print(result.segments)      # ["è¿™æ˜¯äººå·¥æ™ºèƒ½æ¼”ç¤ºã€‚", "å¾ˆé•¿çš„æ–‡æœ¬ä¼šè¢«æ™ºèƒ½åˆ†å‰²ã€‚"]
        >>> print(result.language)      # LanguageType.CHINESE
    """
    return get_global_processor().process(text, split_strategy, target_language)


def batch_process_texts(
    texts: List[str],
    split_strategy: Optional[SplitStrategy] = None,
    target_language: Optional[LanguageType] = None,
) -> List[TextProcessingResult]:
    """
    æ‰¹é‡æ–‡æœ¬å¤„ç†ï¼ˆé«˜æ•ˆå¤„ç†å¤§é‡æ–‡æœ¬ï¼‰

    å¯¹å¤šä¸ªæ–‡æœ¬è¿›è¡Œæ‰¹é‡é¢„å¤„ç†ï¼Œå…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š
    - å¤ç”¨å…¨å±€å¤„ç†å™¨å®ä¾‹ï¼Œå‡å°‘åˆå§‹åŒ–å¼€é”€
    - ç»Ÿä¸€çš„é”™è¯¯å¤„ç†ï¼Œå•ä¸ªæ–‡æœ¬å¤±è´¥ä¸å½±å“å…¶ä»–æ–‡æœ¬
    - é€‚åˆå¤§è§„æ¨¡æ–‡æœ¬å¤„ç†ä»»åŠ¡

    ä½¿ç”¨åœºæ™¯ï¼š
    - æ‰¹é‡å¤„ç†å­—å¹•æ–‡ä»¶
    - å¤§é‡æ–‡æœ¬çš„é¢„å¤„ç†ä»»åŠ¡
    - éœ€è¦ç»Ÿä¸€å¤„ç†å‚æ•°çš„å¤šæ–‡æœ¬åœºæ™¯

    Args:
        texts: æ–‡æœ¬åˆ—è¡¨
        split_strategy: åˆ†å‰²ç­–ç•¥ï¼ˆåº”ç”¨äºæ‰€æœ‰æ–‡æœ¬ï¼‰
        target_language: ç›®æ ‡è¯­è¨€ï¼ˆåº”ç”¨äºæ‰€æœ‰æ–‡æœ¬ï¼‰

    Returns:
        List[TextProcessingResult]: å¤„ç†ç»“æœåˆ—è¡¨ï¼Œä¸è¾“å…¥æ–‡æœ¬ä¸€ä¸€å¯¹åº”

    Example:
        >>> texts = ["æ–‡æœ¬1(æ³¨é‡Š)", "æ–‡æœ¬2[å¤‡æ³¨]", "æ–‡æœ¬3{è¯´æ˜}"]
        >>> results = batch_process_texts(texts)
        >>> for result in results:
        ...     print(result.cleaned_text)
        # "æ–‡æœ¬1"
        # "æ–‡æœ¬2"
        # "æ–‡æœ¬3"
    """
    processor = get_global_processor()
    results = []

    for text in texts:
        try:
            result = processor.process(text, split_strategy, target_language)
            results.append(result)
        except Exception as e:
            # å•ä¸ªæ–‡æœ¬å¤„ç†å¤±è´¥æ—¶ï¼Œåˆ›å»ºé”™è¯¯ç»“æœï¼Œä¸å½±å“å…¶ä»–æ–‡æœ¬
            error_result = TextProcessingResult(
                original_text=text,
                cleaned_text="",
                segments=[],
                language=LanguageType.UNKNOWN,
                text_hash="",
                metadata={"error_type": type(e).__name__},
                is_valid=False,
                error_message=str(e),
            )
            results.append(error_result)

    return results


# ==================== ç®¡ç†å‡½æ•° ====================
